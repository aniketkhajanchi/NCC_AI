<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<meta name="copyright" content="(C) Copyright 2023" />
<meta name="DC.rights.owner" content="(C) Copyright 2023" />
<meta name="generator" content="DITA-OT" /><meta name="DC.type" content="concept" />
<meta name="DC.title" content="Architecture enabler components" />
<meta name="prodname" content="Nokia Converged Charging" />
<meta name="version" content="Release 23.8" />
<meta name="DC.format" content="XHTML" />
<meta name="DC.identifier" content="architecture_enabler_components" />
<link rel="stylesheet" type="text/css" href="../../web/css/commonltr.css" />
<link rel="stylesheet" type="text/css" href="../../web/css/styles.min.css" />
<link rel="stylesheet" type="text/css" href="../../web/css/common-extended.css" /><title>Architecture enabler components</title>
<script src="../web/js/nswtc.js" language="javascript" type="text/javascript">/*(◎_◎;)*/</script>
<script src="../../web/css/../js/script.min.js" language="javascript" type="text/javascript">/*(◎_◎;)*/</script>
</head>
<body id="architecture_enabler_components">
<div class="header"><div id="feedback-link-placeholder" class="feedback-link"></div><hr /></div><h1 class="title topictitle1" id="ariaid-title1">Architecture enabler components</h1>
<div class="body conbody">
        <p class="p">This section describes the enabler components of the NCC architecture.</p>

        <div class="section" id="architecture_enabler_components__section_tyz_rnc_llb"><h2 class="title sectiontitle">Diameter IO handler</h2>
            
            <p class="p">The Diameter IO Handler (IOHD) terminates 4G Diameter traffic and IMS Diameter
                traffic over TCP and communicates with the charging pods over a TCP/mux interface.
                Traffic is load balanced across the pool of available charging pods.</p>

            <p class="p">All HTTP Ingress traffic, including both 5G HTTP/2 as well as the various other HTTP
                traffic, is load balanced directly by the Kubernetes / Istio infrastructure to the
                appropriate pods. There is no IO handler.</p>

        </div>

        <div class="section" id="architecture_enabler_components__section_vcv_jwt_mlb"><h2 class="title sectiontitle">Lock and Transaction Manager</h2><p class="p"> Lock and
                Transaction Manager (LTM) includes two related but distinct functions: Lock Manager
                and Transaction Manager.</p>
<p class="p"><strong class="ph b">Lock Manager</strong></p>
<p class="p">In order to ensure
                consistency of database read/write operations, database users obtain locks from the
                centralized Lock Manager. Since every transaction needs one or more locks, the Lock
                Manager must support very high TPS with very low latency. The Lock Manager supports
                queuing of requests, responses to lock requests, and cleanup of orphaned locks. For
                performance reasons, the Lock Manager keeps all its data in local memory and is
                currently stateful. </p>
<p class="p"><strong class="ph b">Transaction Manager</strong>
            </p>
<p class="p">When a transaction requires updates to multiple records, transaction management
                ensures all records are updated consistently, or not at all, by rolling back
                partially completed transactions. The NCC architectural intent is that many
                transactions involve only a single record and therefore do not require any
                transaction management (which allows for optimum performance).</p>
<p class="p">NCC provides
                its own simple transaction management since it is not available in the database
                itself. NCC provides very simple functionality compared to traditional SQL database
                transaction management. Multiple locks are managed as one transaction with one guard
                timer. Locks can be added to an existing transaction without impacting the original
                guard timer. If the guard timer expires, LTM triggers the Rollback Manager
                (RBM).</p>
LTM clients communicate with the LTM server by using TCP and leveraging
            Istio.</div>

        <div class="section" id="architecture_enabler_components__section_syw_ync_llb"><h2 class="title sectiontitle">Rollback Manager</h2>
            
            <p class="p">The Rollback Manager (RBM) restores the original values to the database records to
                return the database to its pre-transaction state. RBM must have access to previous
                values of data (before the transaction began). When transaction management is
                enabled, the system stores those values into a new Aerospike record for each updated
                record, just before writing the new records. If the transaction succeeds, the old
                value records are deleted. If the transaction fails, the RBM uses the records of old
                values to restore the database back to its pre-transaction state. The records of old
                values are then deleted.</p>

            <p class="p">The rollback component is centralized as only the transactions in the vulnerable
                stage between writing their first and last records need to be rolled back. LTM sends
                requests to the rollback component to roll back orphaned transactions (for example,
                if a requesting application fails and does not complete the transaction) and extends
                the lock time slightly to allow the rollback to complete. For example, if a pod
                fails that was processing 300 TPS lasting less than 100 ms each (&lt; 30
                transactions in progress) and only 33% of the transactions are in the phase where
                rollback is required, then only 10 transactions need to be rolled back.</p>

            <p class="p">Applications might also request a rollback directly if an exception occurs where the
                transaction cannot be completed. For example, if a database error occurs on the
                second or later database write, then the earlier database writes must be rolled
                back.</p>

        </div>

        <div class="section" id="architecture_enabler_components__section_tyw_ync_llb"><h2 class="title sectiontitle">Timed Events Manager</h2>
            
            <p class="p">The Timed Events Manager (TEM) provides a durable, robust and high-performance
                capability for managing the triggering of event processing that must be performed at
                specific times. TEM data is stored in the database to achieve this durability as
                well as replication to the mate NCC system. TEM uses Kafka events to alert an
                application handler of the event trigger. Normally TEM only sends these event
                triggers on the NCC system that is primary for a particular set of subscribers, but
                in the event when the primary is down, the TEM on the secondary system triggers the
                Kafka events for that set of subscribers.</p>

        </div>

        <div class="section" id="architecture_enabler_components__section_cs1_14c_llb"><h2 class="title sectiontitle">Kafka</h2>
            
            <p class="p">Kafka is used in NCC for several purposes.</p>

            <p class="p">Application components can subscribe to data change notification. The database access
                layer publishes the old and new data values in notifications on a publish/subscribe
                topic. That is, any Kafka consumer that subscribes to the topic gets all of the
                messages in the topic, so that all consumers learn about the database change. </p>

            <p class="p">Applications that generate CDRs submit the CDRs to a Kafka work queue. The Kafka
                consumers in that case share the load of reading the CDRs, so that each CDR is read
                by one consumer, that is, each topic partition is assigned to exactly one Kafka
                consumer at all times.</p>

            <p class="p">TEM publishes TEM triggers on Kafka work queues. The consumers get work items such as
                subscriber Life Cycle Management (LCM) events, device audit events, and session
                timeout events and take action.</p>

            <p class="p">Kafka is configured with a replication factor of 2 to provide redundancy with the
                minimal resources used. A good configuration with 3 or more brokers is replication
                factor = 2, minimum in-sync replicas = 1, and acks = all (all in-sync replicas).
                That allows SU to update a broker while writes continue to at least 2 brokers (the
                new leader and one follower). When a broker returns to service, Kafka copies data to
                it that the broker missed to make it in-sync. Once the recovered broker is in-sync,
                it resumes topic partition leadership/followership for the partitions it had before
                the failure (or SU). Setting acks = all helps avoid missed messages during leader
                changes. The new follower does not likely become in-sync during an SU because the
                original leader or follower returns to the cluster within seconds. However, before
                the switch back to the updated broker, that broker is already in-sync and is part of
                acks = all in-sync replicas to allow a seamless transition.</p>

            <p class="p">Software updates for brokers must wait for brokers to return to the in-sync state
                before updating a subsequent broker if rack awareness is not enabled. Given that the
                number of Kafka brokers required for NCC is not large, NCC disables Kafka rack
                awareness and simply wait for brokers to be in-sync before updating the next broker.
                Whether or not NCC uses Kafka rack awareness, NCC requires a Kubernetes operator to
                manage updates for Kafka brokers.</p>

        </div>

        <div class="section" id="architecture_enabler_components__section_ycz_5rc_llb"><h2 class="title sectiontitle">Database</h2>
            
            <p class="p">NCC uses a high-performance clustered in-memory NoSQL database from Aerospike. It
                supports advanced data types with nesting (NCC uses lists, maps, lists of maps, and
                more), N-way geographic replication between sites (clusters), flexible data
                migration within the cluster to maintain the requested replication factor,
                tombstoning to prevent resurrection of deleted records, and many other advanced
                features.</p>

            <p class="p">NCC uses a replication factor of 2 for all namespaces, meaning that Aerospike ensures
                that two copies of every record always exist within the cluster. When a database pod
                fails, data migration happens within the cluster to add a copy of those records onto
                the other database pods. When that failed database pod recovers, the records are
                migrated back onto that recovered pod again.</p>

            <p class="p">Database pods are horizontally scalable, where dimensioning in some deployments is
                driven by the need for more TPS, and in other deployments by the need for more
                memory. Note that database pods cannot scale in and scale out as freely as most
                other container types, since even in times of low traffic TPS, the database records
                still require a similar amount of memory. This creates a minimum number of database
                pod instances, independent of traffic load. For this reason, the Aerospike database
                pods are manually scalable at this time, rather than automatically scalable.</p>

            <p class="p">Most NCC records are 2-way replicated in the operator's network, using Aerospike XDR,
                with a mate NCC system. (Two copies of a record within a system's local database
                cluster, replicated with another system where there are also two copies in that
                cluster.) The global mapping records are N-way replicated in the network, if the
                operator is using a Networked NCC Pair deployment model.</p>

            <p class="p">NCC leverages the Strong Consistency feature of Aerospike to ensure data integrity
                within the local cluster. If any data partition is unavailable (for example, from
                multiple database pods failing at the same time before data migration can complete),
                the database service is marked out-of-service and the local system no longer accepts
                incoming traffic. If a recovering database pod has been down for longer than six
                hours (the default tombraider time for cleaning up tombstones for deleted records),
                NCC automatically deletes that node's Aerospike disk files and forces a clean boot
                to ensure data integrity, where the recovering node must learn of all data from its
                peers in the local Aerospike cluster. If nodes in both racks must do a clean boot,
                alarms are raised and a disaster recovery procedure is required, since removing disk
                files on both racks removes all local copies of a subset of records. For more
                information about monitoring the Aerospike database partition and roster status, see
                the <cite class="cite">db-roster</cite> tool topic in the <cite class="cite">Operations, Administration, and
                    Maintenance Guide</cite>.</p>

            <p class="p">The database layer built into the pods of database clients supports a pub/sub feature
                that allows clients to subscribe to notifications of database record changes. Kafka
                is used as the pub/sub provider. The database layer inserts into Kafka, for
                subscribed changes, the before and after values for the changed record.</p>

            <p class="p"><strong class="ph b">Database zones, rack awareness, and database software updates</strong></p>

            <p class="p">Aerospike supports rack awareness, a feature that NCC uses to configure one-half of
                its database pod instances in rack 1 and the other half in rack 2. Aerospike keeps
                one copy of a record in rack 1 and the other copy in rack 2.</p>

            <p class="p">Rack awareness provides some improvement to NCC availability, as it allows an NCC
                system to lose multiple database instances in the same rack in rapid fire, without
                losing any data access. Without rack awareness, losing any two database instances
                before migration completes could cause loss of some data access and would result in
                NCC monitoring declaring this system OOS (till the instances recover).</p>

        </div>

        <div class="section" id="architecture_enabler_components__section_ftd_jfx_54b"><h2 class="title sectiontitle">Backup and restore</h2>
            
            <p class="p">NCC provides backup and restore capabilities for various use cases that include
                disaster recovery. Backup policies are configured to control the backup operations
                and to configure a particular backup destination or target. NCC supports a variety
                of backup destinations and options. Contact Nokia for more information about
                integrating backup and restore components into specific environments.</p>

        </div>

        <div class="section" id="architecture_enabler_components__section_vj4_1dn_hpb"><h2 class="title sectiontitle">Communication security</h2>
            
            <p class="p">NCC supports mTLS for intersite Aerospike database replication (XDR) between
                geo-redundant NCC systems. NCC leverages Istio infrastructure in Kubernetes,
                including certificate management, to communicate between database clusters for XDR.
                The mTLS intersite communication for XDR is a configurable option and can be turned
                on or off. However, there is a brief traffic interruption during this transition. </p>

            <p class="p">External communication between NCC systems also supports mTLS for MariaDB replication
                and Diameter site routing.</p>

            <p class="p">NCC supports mTLS for intrasite communication between Aerospike database pods, and
                between Aerospike database pods and application pods within a local cluster (site).
                The mTLS intrasite communication is a configurable option and can be turned on or
                off. For more information about Aerospike communication using TLS and mTLS, see
                    <a class="xref" href="cnf_communication_matrix.html">Communication matrix</a> and the <cite class="cite">Operation,
                    Administration, and Maintenance Guide</cite> in the NCC documentation set.</p>

            <p class="p">Istio mTLS can be enabled for internal communications, which many deployments
                require. The CPU cost is significant to encrypt internal communications and is
                factored into NCC dimensioning. For most pod types, the Istio proxy provides mTLS
                without knowledge or involvement of the application containers. </p>

            <p class="p">NCC provides support for external Diameter over TLS. For HTTP traffic, any encryption
                is up to the infrastructure provided by the operator.</p>

        </div>

        <div class="section"><h2 class="title sectiontitle">Tomb Raider Scheduler (TRS)</h2>
            
            <div class="p">The NCC Tomb Raider Scheduler (TRS) is a process that runs on the active central
                management pod and schedules the run of the tomb-raider on all Aerospike server
                nodes in a staggered way to lower the burst of disk IOPS that would have been caused
                had the TR been run at the same time on all namespaces on all the nodes. TR cycle
                time is configurable (default 24 hours) and the tomb-raider-sleep is calculated
                dynamically according to the installation's disk sizes and the TR cycle time. for
                more information on the TR cycle time, see the <cite class="cite"><span class="ph">NCC</span></cite> Global Configuration Guide.<div class="note"><div class="notetitle"><img src="../../web/css/../images/note.svg" title="note-icon" class="noteicon" />Note:</div> <ul class="ul">
                        <li class="li">After you update the Tomb Raider Cycle time, it is effective after the
                            already scheduled next cycle time or the switchover happens. For
                            example, Tomb Raider Cycle Time is set as: 24 hours. The last TR ran on
                            26th Feb 2023 at 16:00 hrs and next TR cycle is scheduled to occur on
                            27th Feb 2023 at 16:00 hrs. But on 27th Feb 2023 at 06:00 hrs, you
                            update the Tomb Raider Cycle Time to 6 hours. However, the next TR still
                            occurs at 27th Feb 2023 at 16:00 hrs (as per already scheduled time) and
                            the next subsequent TR cycle gets scheduled for 27th Feb 2023 22:00 hrs
                            (for example, after 6 hours).</li>

                        <li class="li">There are some observations that TR process eventually completes before
                            the configured TR cycle time. This is due to the Tomb-raider not reading
                            all the disk blocks on disk files. Instead, it just reads the non-free
                            blocks of the disk file. Typically, in a stable state, the free-blocks
                            are about 80%-90% of the total blocks.</li>

                    </ul>
</div>
</div>

        </div>

    </div>
<div class="footer"><hr /><p>Overview Guide • P556766-DN1000054925-R23.8 • 1 • <a href="../home.html">Cover</a> • ©2023 Nokia. Nokia Confidential Information • Use subject to agreed restrictions on disclosure and use.</p></div></body>
</html>